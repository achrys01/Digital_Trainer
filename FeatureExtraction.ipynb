{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy import signal\n",
    "from scipy.interpolate import interp1d\n",
    "from ahrs.filters import Madgwick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change root directory\n",
    "dirPath = r'C:\\Users\\Andreas\\Desktop\\Thesis\\Data'\n",
    "os.chdir(dirPath)\n",
    "print('Current working directory: {0}'.format(os.getcwd()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv_files(dirPath = dirPath, sample = 'all', exercise = 'all'):\n",
    "    root = dirPath+'\\\\'+'Labelled'\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "    for samplefolder in os.listdir(root):\n",
    "        if sample in samplefolder or sample == 'all':\n",
    "\n",
    "            for exercisefolder in os.listdir(root+'\\\\'+samplefolder):\n",
    "                if exercise in exercisefolder or exercise == 'all':\n",
    "                    \n",
    "                    for file in os.listdir(root+'\\\\'+samplefolder+'\\\\'+exercisefolder):\n",
    "                        if '.csv' in file:\n",
    "                            csv_path = root+'\\\\'+samplefolder+'\\\\'+exercisefolder+'\\\\'+file\n",
    "                            data = pd.read_csv(csv_path, delimiter=',', encoding = 'utf-8')\n",
    "                            df = pd.concat([df, data], ignore_index=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv(csv, dirPath = dirPath):\n",
    "                    \n",
    "    for file in os.listdir(dirPath):\n",
    "        if csv in file:\n",
    "            new_df = pd.read_csv(file, delimiter=',', encoding = 'utf-8')\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def restructure_df(df):\n",
    "\n",
    "    csv_df = df.copy()\n",
    "    new_df = pd.DataFrame()\n",
    "    # Drop video timestamp\n",
    "    csv_df.drop(columns='time', inplace = True)\n",
    "    csv_df.drop(csv_df[csv_df.SensorName == 'video'].index, inplace = True)\n",
    "    # Drop not labelled data\n",
    "    csv_df.drop(csv_df[csv_df.label == 'Unassigned'].index, inplace = True)\n",
    "    csv_df.drop(csv_df[csv_df.label == 'idle'].index, inplace = True)\n",
    "    \n",
    "    # Group by recording\n",
    "    groups = csv_df.groupby(['Subject','Exercise'])\n",
    "    for _, dfGroup in groups:\n",
    "        \n",
    "        data = pd.DataFrame()\n",
    "        i=0\n",
    "\n",
    "        merge_groups = dfGroup.groupby(['DataType','SensorName'])\n",
    "        for [strDatatype,strSensorName], dfDatatype in merge_groups:\n",
    "\n",
    "            dfDatatype.reset_index(inplace=True,drop=True)\n",
    "\n",
    "            # Create new columns for different sensors\n",
    "            for _,axis in enumerate(['x', 'y', 'z']):\n",
    "                col_name = f'{strSensorName}_{strDatatype}_{axis}'\n",
    "                dfDatatype[col_name] = dfDatatype[axis]\n",
    "            \n",
    "            # Drop unnecesary columns\n",
    "            dfDatatype.drop(columns=['x','y','z'], inplace = True)\n",
    "\n",
    "            # First iteration\n",
    "            if i == 0:\n",
    "                data = dfDatatype\n",
    "                i+=1\n",
    "            else:\n",
    "                # Difference between different sensor timestamps\n",
    "                diff = np.abs(data.t.values.reshape(-1, 1) - dfDatatype.t.values)\n",
    "                # Index of the nearest t\n",
    "                nearest = diff.argmin(axis=0)\n",
    "                dfDatatype.t = data.t.iloc[nearest].values\n",
    "                \n",
    "                for col in dfDatatype.columns.values: \n",
    "                    if col not in data.columns.values:\n",
    "                        data[col] = dfDatatype[col]\n",
    "\n",
    "        # Drop possible extra timestamp indices\n",
    "        data.dropna(inplace=True)\n",
    "\n",
    "        # Reset time for each rep\n",
    "        for _,rep in enumerate(data.rep.unique()):\n",
    "            # Find rep start\n",
    "            rep_start = data.t.loc[(data.rep == rep).argmax()]\n",
    "            # Assign difference to list\n",
    "            t = []\n",
    "            for value in data.t.loc[(data.rep == rep)]:\n",
    "                t.append(value-rep_start)\n",
    "            # Change value\n",
    "            data.loc[(data.rep == rep), 't'] = np.around(np.array(t),2)\n",
    "        \n",
    "        new_df = pd.concat([new_df, data],ignore_index=True)\n",
    "    \n",
    "    # Replace partitioned exercise\n",
    "    new_df['Exercise'] = new_df['Exercise'].str.replace(' 1', '').str.replace(' 2', '')\n",
    "    # Drop reduntant columns\n",
    "    new_df.drop(columns=['DataType','SensorName'], inplace = True)\n",
    "    # Drop miscalculated timestamps\n",
    "    new_df = new_df.loc[new_df.t < 4]\n",
    "    #Transform rep to string\n",
    "    new_df['rep'] = new_df['rep'].astype(str)\n",
    "    # Drop rows with missing sensor data\n",
    "    new_df.dropna(inplace=True)\n",
    "\n",
    "    new_df.to_csv('raw_dataframe_'+time.strftime(\"%Y%m%d_%H%M\")+'.csv', sep=',', index=False)\n",
    "\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers(data):\n",
    "\n",
    "    clean_df = pd.DataFrame()\n",
    "    dirty_df = pd.DataFrame()\n",
    "\n",
    "    # Group by recording\n",
    "    groups = data.groupby(['Subject','Exercise'])\n",
    "    for strGroup, dfGroup in groups:\n",
    "\n",
    "        rep_groups = dfGroup.groupby('rep')\n",
    "        for strRep, dfRep in rep_groups:\n",
    "\n",
    "            tmean = dfRep['t'].mean()\n",
    "            tstd = dfRep['t'].std()\n",
    "            threshold = 3 * tstd\n",
    "            dfRep = dfRep[(dfRep['t'] - tmean).abs() < threshold]\n",
    "\n",
    "            for col in dfRep.iloc[:,5:]:\n",
    "                colmean = dfRep[col].mean()\n",
    "                colstd = dfRep[col].std()\n",
    "                threshold = 3*colstd\n",
    "                # cutRep = dfRep[(dfRep[col] - colmean).abs() > threshold]\n",
    "                cleanRep = dfRep[(dfRep[col] - colmean).abs() < threshold]\n",
    "\n",
    "            clean_df = pd.concat([clean_df,cleanRep],ignore_index=True)\n",
    "            # dirty_df = pd.concat([dirty_df,cutRep],ignore_index=True)        \n",
    "    \n",
    "    clean_df.dropna()\n",
    "    clean_df.to_csv('clean_dataframe_'+time.strftime(\"%Y%m%d_%H%M\")+'.csv', sep=',', index=False)\n",
    "\n",
    "    return clean_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample(data):\n",
    "\n",
    "    resampled_df = pd.DataFrame()\n",
    "\n",
    "    # Group by recording\n",
    "    groups = data.groupby(['Subject','Exercise'])\n",
    "    for strGroup, dfGroup in groups:\n",
    "\n",
    "        rep_groups = dfGroup.groupby('rep')\n",
    "        for strRep, dfRep in rep_groups:\n",
    "\n",
    "            #Target time window\n",
    "            sample_window = 3\n",
    "            # Target frequency\n",
    "            sample_f = 50\n",
    "            # Desired number of samples\n",
    "            num_samples = sample_window * sample_f\n",
    "\n",
    "            dfRep.reset_index(inplace=True,drop=True)\n",
    "            resampled = pd.DataFrame()\n",
    "            for col in dfRep.iloc[:,5:]:\n",
    "\n",
    "                x_old = np.linspace(0, 1, len(dfRep[col]))\n",
    "                f = interp1d(x_old, dfRep[col], kind='linear')\n",
    "                \n",
    "                x_new = np.linspace(0, 1, num_samples)\n",
    "                resampled[col] = f(x_new)\n",
    "\n",
    "            resampled.insert(0,'Subject', strGroup[0])\n",
    "            resampled.insert(1,'Exercise', strGroup[1])\n",
    "            resampled.insert(2,'rep', strRep)\n",
    "            resampled.insert(3,'t', np.linspace(0, sample_window, num_samples))\n",
    "            resampled.insert(4,'label', dfRep.loc[0,'label'])\n",
    "\n",
    "            resampled_df = pd.concat([resampled_df,resampled],ignore_index=True)\n",
    "            \n",
    "    resampled_df.to_csv('resampled_dataframe_'+time.strftime(\"%Y%m%d_%H%M\")+'.csv', sep=',', index=False)\n",
    "\n",
    "    return resampled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def butterworth_filter(data, cutoff_frequency, fs, order):\n",
    "    \n",
    "    nyquist_frequency = 0.5 * fs\n",
    "\n",
    "    # Group by recording\n",
    "    groups = data.groupby(['Subject','Exercise'])\n",
    "    for strGroup, dfGroup in groups:\n",
    "        # Group by rep\n",
    "        rep_groups = dfGroup.groupby('rep')\n",
    "        for strRep, dfRep in rep_groups:\n",
    "            dfRep.reset_index(inplace=True,drop=True)\n",
    "\n",
    "            for col in dfRep.iloc[:,5:]:\n",
    "\n",
    "                if 'Acc' in col:\n",
    "                    normalized_cutoff_frequency = cutoff_frequency[0] / nyquist_frequency\n",
    "                    filter_type = 'low'\n",
    "                    b, a = signal.butter(order, normalized_cutoff_frequency, btype=filter_type, analog=False)\n",
    "\n",
    "                    data.loc[(data.Subject == strGroup[0]) & (data.Exercise == strGroup[1]) & (data.rep == strRep), col] = signal.filtfilt(b, a, dfRep[col])\n",
    "\n",
    "                elif 'Gyro' in col:\n",
    "                    normalized_cutoff_frequency = cutoff_frequency[1] / nyquist_frequency\n",
    "                    filter_type = 'high'\n",
    "                    b1, a1 = signal.butter(order, normalized_cutoff_frequency, btype=filter_type, analog=False)\n",
    "                    \n",
    "                    normalized_cutoff_frequency = cutoff_frequency[0] / nyquist_frequency\n",
    "                    filter_type = 'low'\n",
    "                    b2, a2 = signal.butter(order, normalized_cutoff_frequency, btype=filter_type, analog=False)\n",
    "\n",
    "                    data.loc[(data.Subject == strGroup[0]) & (data.Exercise == strGroup[1]) & (data.rep == strRep), col] = signal.filtfilt(b1, a1, dfRep[col])\n",
    "                    data.loc[(data.Subject == strGroup[0]) & (data.Exercise == strGroup[1]) &  (data.rep == strRep), col] = signal.filtfilt(b2, a2, dfRep[col])\n",
    "\n",
    "    data.to_csv('butter_dataframe_'+time.strftime(\"%Y%m%d_%H%M\")+'.csv', sep=',', index=False)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complementary_filter(data):\n",
    "    \n",
    "    comp_df = pd.DataFrame()\n",
    "\n",
    "    # Group by recording\n",
    "    groups = data.groupby(['Subject','Exercise'])\n",
    "    for strGroup, dfGroup in groups:\n",
    "        # Group by rep\n",
    "        rep_groups = dfGroup.groupby('rep')\n",
    "        for strRep, dfRep in rep_groups:\n",
    "            dfRep.reset_index(inplace=True,drop=True)\n",
    "\n",
    "            result = pd.DataFrame()\n",
    "            df = dfRep.iloc[:,5:]\n",
    "            \n",
    "            for col in df.columns:\n",
    "    \n",
    "                sensor_name, _, _ = col.split('_')\n",
    "\n",
    "                acc = np.array([df[sensor_name + '_Acc_x'], df[sensor_name + '_Acc_y'], df[sensor_name + '_Acc_z']])\n",
    "                gyro = np.array([df[sensor_name + '_Gyro_x'], df[sensor_name + '_Gyro_y'], df[sensor_name + '_Gyro_z']])\n",
    "\n",
    "                madgwick = Madgwick(gyr=gyro.T, acc=acc.T, frequency=50)\n",
    "                \n",
    "                # Assign quaternions\n",
    "                for q in range(madgwick.Q.shape[1]):\n",
    "                    result[sensor_name + f'_q{q}'] = madgwick.Q[:,q]\n",
    "            \n",
    "            result.insert(0,'Subject', strGroup[0])\n",
    "            result.insert(1,'Exercise', strGroup[1])\n",
    "            result.insert(2,'rep', strRep)\n",
    "            result.insert(3,'t', dfRep['t'])\n",
    "            result.insert(4,'label', dfRep.loc[0,'label'])\n",
    "             \n",
    "            comp_df = pd.concat([comp_df, result], ignore_index=True)\n",
    "    \n",
    "    comp_df.to_csv('complementary_dataframe_'+time.strftime(\"%Y%m%d_%H%M\")+'.csv', sep=',', index=False)\n",
    "    return comp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_datapoints(data):\n",
    "\n",
    "    # Group by recording\n",
    "    groups = data.groupby(['Exercise','Subject'])\n",
    "    for strGroup, dfGroup in groups:\n",
    "        print(strGroup)\n",
    "        fig, ax = plt.subplots(nrows=1, ncols=3, layout=\"constrained\", figsize=(30,10))\n",
    "        for col in dfGroup.iloc[:,5:]:\n",
    "            if 'Acc' in col:\n",
    "                if '_x' in col:\n",
    "                    j=0\n",
    "                elif '_y' in col:\n",
    "                    j=1\n",
    "                elif '_z' in col:\n",
    "                    j=2\n",
    "                dfGroup.sort_values(by = 't')\n",
    "                ax[j].scatter(dfGroup['t'], dfGroup[col], marker = '.', s = 10 , label = col)\n",
    "                ax[j].legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_filters(raw, resampled, butter):\n",
    "\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=2, layout=\"constrained\", figsize=(15,6))\n",
    "    for df in [raw, resampled, butter]:\n",
    "        acc = df.loc[(df.label == 'Correct repetition')&(df.rep == 17)&(df.Exercise == 'Squats')&(df.Subject == 'Subject_6'), ['t','Lumbar_Acc_z']]\n",
    "        gyro = df.loc[(df.label == 'Correct repetition')&(df.rep == 17)&(df.Exercise == 'Squats')&(df.Subject == 'Subject_6'), ['t','Lumbar_Gyro_z']]\n",
    "\n",
    "        ax[0].plot(acc['t'], acc['Lumbar_Acc_z'])\n",
    "        ax[1].plot(gyro['t'], gyro['Lumbar_Gyro_z'])\n",
    "        \n",
    "        ax[0].set_title('Accelerometer filtering sample')\n",
    "        ax[0].set_ylabel('Acceleration (g)')\n",
    "        ax[0].set_xlabel('Time (s)')\n",
    "\n",
    "        ax[1].set_title('Gyroscope filtering sample')\n",
    "        ax[1].set_ylabel('Angular velocity (rad/s)')\n",
    "        ax[1].set_xlabel('Time (s)')\n",
    "\n",
    "        ax[0].legend(['Raw data', 'Resampled data', 'Butterworth filter'], loc='upper right')\n",
    "        ax[1].legend(['Raw data', 'Resampled data', 'Butterworth filter'], loc='upper right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_quaternions(data):\n",
    "    # Group by recording\n",
    "    groups = data.groupby(['Subject','Exercise','rep'])\n",
    "    for strGroup, dfGroup in groups:\n",
    "        if strGroup[1] == 'Lunges' and strGroup[2] == 7:\n",
    "            fig, ax = plt.subplots(nrows=1, ncols=4, layout=\"constrained\", figsize=(30,6))\n",
    "            for col in dfGroup.iloc[:,5:]:\n",
    "                    if 'q0' in col:\n",
    "                        ax[0].plot(dfGroup['t'], dfGroup[col], label = col)\n",
    "                        ax[0].set_ylabel('q0')\n",
    "                        ax[0].set_xlabel('time (s)')\n",
    "                        ax[0].set_title(\"Quaternion 0\")\n",
    "                    elif 'q1' in col:\n",
    "                        ax[1].plot(dfGroup['t'], dfGroup[col], label = col)\n",
    "                        ax[1].set_ylabel('q1')\n",
    "                        ax[1].set_xlabel('time (s)')\n",
    "                        ax[1].set_title(\"Quaternion 1\")\n",
    "                    elif 'q2' in col:\n",
    "                        ax[2].plot(dfGroup['t'], dfGroup[col], label = col)\n",
    "                        ax[2].set_ylabel('q2')\n",
    "                        ax[2].set_xlabel('time (s)')\n",
    "                        ax[2].set_title(\"Quaternion 2\")\n",
    "                    elif 'q3' in col:\n",
    "                        ax[3].plot(dfGroup['t'], dfGroup[col], label = col)\n",
    "                        ax[3].set_ylabel('q3')\n",
    "                        ax[3].set_xlabel('time (s)')\n",
    "                        ax[3].set_title(\"Quaternion 3\")\n",
    "                        \n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_angles_correlation(df, exercise = 'all'):\n",
    "\n",
    "    if exercise in df.Exercise.values:\n",
    "        df = df.groupby('Exercise').get_group(exercise)\n",
    "    \n",
    "    df = df.iloc[:,5:]\n",
    "    # Calculate correlation matrix\n",
    "    corr_matrix = df.corr()\n",
    "    # Plot the correlation matrix as a heatmap\n",
    "    fig, ax = plt.subplots(figsize=(10,10))\n",
    "    im = ax.imshow(corr_matrix, cmap='coolwarm', vmin=-1, vmax = 1)\n",
    "\n",
    "    # Add the correlation values to the heatmap\n",
    "    for i in range(len(corr_matrix)):\n",
    "        for j in range(len(corr_matrix)):\n",
    "            ax.text(j, i, round(corr_matrix.iloc[i, j], 2),\n",
    "                        ha=\"center\", va=\"center\", color=\"black\")\n",
    "\n",
    "    # Set the axis labels\n",
    "    ax.set_xticks(range(len(df.columns)))\n",
    "    ax.set_yticks(range(len(df.columns)))\n",
    "    ax.set_xticklabels(df.columns)\n",
    "    ax.set_yticklabels(df.columns)\n",
    "\n",
    "    # Rotate the x-axis labels\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "            rotation_mode=\"anchor\")\n",
    "\n",
    "    # Add a title\n",
    "    ax.set_title(f'Quaternions - {exercise}')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_correlation(df, exercise = 'all'):\n",
    "\n",
    "    if exercise in df.Exercise.values:\n",
    "        df = df.groupby('Exercise').get_group(exercise)\n",
    "    \n",
    "    df = df.iloc[:,5:]\n",
    "    corrdf = pd.DataFrame()\n",
    "\n",
    "    for col in df.columns:\n",
    "        sensor_name, sensor_type,_ = col.split('_')\n",
    "        # Find magnitude to determine correlation (instead of using each axis)\n",
    "        magnitude = np.sqrt(df[sensor_name + '_' + sensor_type + '_' + 'x']**2 +\n",
    "                            df[sensor_name + '_' + sensor_type + '_' + 'y']**2 +\n",
    "                            df[sensor_name + '_' + sensor_type + '_' + 'z']**2)\n",
    "        \n",
    "        corrdf[sensor_name + '_' + sensor_type] = magnitude\n",
    "    \n",
    "    # Calculate correlation matrix\n",
    "    corr_matrix = corrdf.corr()\n",
    "    # Plot the correlation matrix as a heatmap\n",
    "    fig, ax = plt.subplots(figsize=(10,10))\n",
    "    im = ax.imshow(corr_matrix, cmap='coolwarm', vmin=-1, vmax = 1)\n",
    "\n",
    "    # Add the correlation values to the heatmap\n",
    "    for i in range(len(corr_matrix)):\n",
    "        for j in range(len(corr_matrix)):\n",
    "            ax.text(j, i, round(corr_matrix.iloc[i, j], 2),\n",
    "                        ha=\"center\", va=\"center\", color=\"black\")\n",
    "\n",
    "    # Set the axis labels\n",
    "    ax.set_xticks(range(len(corrdf.columns)))\n",
    "    ax.set_yticks(range(len(corrdf.columns)))\n",
    "    ax.set_xticklabels(corrdf.columns)\n",
    "    ax.set_yticklabels(corrdf.columns)\n",
    "\n",
    "    # Rotate the x-axis labels\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "            rotation_mode=\"anchor\")\n",
    "\n",
    "    # Add a title\n",
    "    ax.set_title(f'Resampled signal - {exercise}')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics_correlation(df, exercise = 'all', sensor = 'all'):\n",
    "\n",
    "    if exercise in df.Exercise.values:\n",
    "        df = df.groupby('Exercise').get_group(exercise)\n",
    "        \n",
    "    if df.columns.str.contains(sensor).any():\n",
    "        df = df.filter(like=sensor)\n",
    "    \n",
    "    df = df.iloc[:,4:]\n",
    "    corrdf = pd.DataFrame()\n",
    "\n",
    "    for col in df.columns:\n",
    "        sensor_name, sensor_type, axis, metric = col.split('_')\n",
    "        \n",
    "        if axis in ['x','y','z']:\n",
    "            magnitude = np.sqrt(df['Lumbar_' + sensor_type + '_' + 'x'+ '_' + metric]**2 +\n",
    "                                df['Lumbar_' + sensor_type + '_' + 'y'+ '_' + metric]**2 +\n",
    "                                df['Lumbar_' + sensor_type + '_' + 'z'+ '_' + metric]**2)\n",
    "            corrdf['Lumbar_'+sensor_type+ '_' + metric] = magnitude\n",
    "\n",
    "    corr_matrix = corrdf.corr()\n",
    "    # Plot the correlation matrix as a heatmap\n",
    "    fig, ax = plt.subplots(figsize=(10,10))\n",
    "    im = ax.imshow(corr_matrix, cmap='coolwarm', vmin=-1, vmax = 1)\n",
    "\n",
    "    # Add the correlation values to the heatmap\n",
    "    for i in range(len(corr_matrix)):\n",
    "        for j in range(len(corr_matrix)):\n",
    "            ax.text(j, i, round(corr_matrix.iloc[i, j], 2),\n",
    "                        ha=\"center\", va=\"center\", color=\"black\")\n",
    "\n",
    "    # Set the axis labels\n",
    "    ax.set_xticks(range(len(corrdf.columns)))\n",
    "    ax.set_yticks(range(len(corrdf.columns)))\n",
    "    ax.set_xticklabels(corrdf.columns)\n",
    "    ax.set_yticklabels(corrdf.columns)\n",
    "\n",
    "    # Rotate the x-axis labels\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "            rotation_mode=\"anchor\")\n",
    "\n",
    "    # Add a title\n",
    "    ax.set_title(f'Statistical features - {exercise}')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def statistical_features(df,name):\n",
    "\n",
    "    sdf = pd.DataFrame()\n",
    "\n",
    "    # wrlds \"normalization\"\n",
    "#     for col in df.columns:\n",
    "#          if 'Acc' in col:\n",
    "#               df[col] = df[col]/16\n",
    "#          elif 'Gyro' in col:\n",
    "#               df[col] = df[col]/2000\n",
    "    \n",
    "    # Group by recording\n",
    "    groups = df.groupby(['Subject','Exercise'])\n",
    "    for strGroup, dfGroup in groups:\n",
    "     rep_groups = dfGroup.groupby('rep')\n",
    "     for strRep, dfRep in rep_groups:\n",
    "          dfRep.reset_index(inplace=True,drop=True)\n",
    "\n",
    "          maxdf = pd.DataFrame(dfRep.iloc[:,5:].max().add_suffix('_max')).T\n",
    "          mindf = pd.DataFrame(dfRep.iloc[:,5:].min().add_suffix('_min')).T\n",
    "          mediandf = pd.DataFrame(dfRep.iloc[:,5:].median().add_suffix('_median')).T\n",
    "          meandf = pd.DataFrame(dfRep.iloc[:,5:].mean().add_suffix('_mean')).T\n",
    "          stddf = pd.DataFrame(dfRep.iloc[:,5:].std().add_suffix('_std')).T\n",
    "          kurtdf = pd.DataFrame(dfRep.iloc[:,5:].kurtosis().add_suffix('_kurtosis')).T\n",
    "          skewdf = pd.DataFrame(dfRep.iloc[:,5:].skew().add_suffix('_skewness')).T\n",
    "\n",
    "          repdf = pd.concat([maxdf,mindf,mediandf,\n",
    "                              meandf,stddf,\n",
    "                              kurtdf,skewdf],\n",
    "                              axis = 1)\n",
    "          repdf.insert(0,'Subject', strGroup[0])\n",
    "          repdf.insert(1,'Exercise', strGroup[1])\n",
    "          repdf.insert(2,'label', dfRep.loc[0,'label'])\n",
    "\n",
    "          sdf = pd.concat([sdf, repdf],ignore_index=True)\n",
    "\n",
    "    sdf.to_csv(name+time.strftime(\"%Y%m%d_%H%M\")+'.csv', sep=',', index=False)\n",
    "    return sdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features(df,name):\n",
    "\n",
    "    asdf = pd.DataFrame()\n",
    "    \n",
    "    # Group by recording\n",
    "    groups = df.groupby(['Subject','Exercise'])\n",
    "    for strGroup, dfGroup in groups:\n",
    "\n",
    "        rep_groups = dfGroup.groupby('rep')\n",
    "        for strRep, dfRep in rep_groups:\n",
    "            dfRep.reset_index(inplace=True,drop=True)\n",
    "\n",
    "            repdf = pd.DataFrame(dfRep.loc[dfRep.index[0]]).T\n",
    "\n",
    "            for col in dfRep.iloc[:,5:]:\n",
    "                repdf.at[0, col] = dfRep[col].tolist()\n",
    "   \n",
    "            asdf = pd.concat([asdf, repdf],ignore_index=True)\n",
    "    \n",
    "    \n",
    "    asdf.drop(columns=['rep','t'], inplace = True)\n",
    "    asdf.to_csv(name+time.strftime(\"%Y%m%d_%H%M\")+'.csv', sep=',', index=False)\n",
    "    return asdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = read_csv_files()\n",
    "new_df = restructure_df(df)\n",
    "# clean_df = remove_outliers(new_df)\n",
    "# resampled_df = resample(clean_df)\n",
    "plot_datapoints(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Butterworth\n",
    "resampled_df = read_csv('resampled_dataframe_')\n",
    "\n",
    "# Parameters\n",
    "cutoff_frequency = [3, 0.5]\n",
    "fs = 50\n",
    "order = 2\n",
    "\n",
    "butter_df = butterworth_filter(resampled_df, cutoff_frequency, fs, order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = read_csv('clean_dataframe')\n",
    "resampled = read_csv('resampled_dataframe')\n",
    "butter = read_csv('butter_dataframe')\n",
    "plot_filters(raw, resampled, butter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "butter = read_csv('butter_dataframe')\n",
    "compl = complementary_filter(butter)\n",
    "plot_quaternions(compl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = read_csv('raw_dataframe')\n",
    "resampled = read_csv('resampled_dataframe')\n",
    "butter = read_csv('butter_dataframe')\n",
    "\n",
    "raw_stats = statistical_features(raw,'raw_statistical_features_')\n",
    "res_stats = statistical_features(resampled,'resampled_statistical_features_')\n",
    "filt_stats = statistical_features(butter,'filtered_statistical_features_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resampled = read_csv('resampled_dataframe')\n",
    "butter = read_csv('butter_dataframe')\n",
    "compl = read_csv('complementary_dataframe')\n",
    "\n",
    "rdf = features(resampled,'resampled_features_')\n",
    "fdf = features(butter,'butter_features_')\n",
    "adf = features(compl,'angles_features_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resampled = read_csv('resampled_dataframe_')\n",
    "compl = read_csv('complementary_dataframe_')\n",
    "stats = read_csv('statistical_features_')\n",
    "\n",
    "plot_correlation(resampled)\n",
    "plot_angles_correlation(compl)\n",
    "plot_metrics_correlation(stats)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "18191ca4c8cb7fb833edee195c9ea59f5ce50831c61b24d0d25301471638634c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
